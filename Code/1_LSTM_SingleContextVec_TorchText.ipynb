{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Ass_phase1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK2kj6qucJL3"
      },
      "source": [
        "import csv\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLth0pEXdtfS",
        "outputId": "37edcac7-5501-439b-9640-6fd6aa2573e8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nKE4hMlWXVQ"
      },
      "source": [
        "## Pre-processing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRg3kwmCWdCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "208f7a46-d405-4d95-a51b-0b194b72dc35"
      },
      "source": [
        "train_set=[]\n",
        "i=0\n",
        "with open('/content/gdrive/My Drive/Hindi to English MT/train.csv', 'r') as f:\n",
        "    csv_reader = csv.reader(f, delimiter=',')\n",
        "    for row in csv_reader:\n",
        "      if (i==0):\n",
        "        i+=1\n",
        "        continue\n",
        "      train_set.append([row[1],row[2].lower()])\n",
        "train_set[0:10] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध से वापसी ली, उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।',\n",
              "  \"in el salvador, both sides that withdrew from their civil war took moves that had been proven to mirror a prisoner's dilemma strategy.\"],\n",
              " ['मैं उनके साथ कोई लेना देना नहीं है.', 'i have nothing to do with them.'],\n",
              " ['-हटाओ रिक.', 'fuck them, rick.'],\n",
              " ['क्योंकि यह एक खुशियों भरी फ़िल्म है.', \"because it's a happy film.\"],\n",
              " ['The thought reaching the eyes...', 'the thought reaching the eyes...'],\n",
              " ['मैंने तुमे School से हटवा दिया .', 'i got you suspended.'],\n",
              " ['यह Vika, एक फूल है.', \"it's a flower, vika.\"],\n",
              " ['पर मेरे लिए उसका यहुदी विरोधी होना उसके कार्यों को और भी प्रशंसनीय बनाता है क्योंकि उसके पास भी पक्षपात करने के वही कारण थे जो बाकी फौजियों के पास थे पर उसकी सच जानने और उसे बनाए रखने की प्रेरणा सबसे ऊपर थी',\n",
              "  'but personally, for me, the fact that picquart was anti-semitic actually makes his actions more admirable, because he had the same prejudices, the same reasons to be biased as his fellow officers, but his motivation to find the truth and uphold it trumped all of that.'],\n",
              " ['नहीं, नहीं, नहीं... ठीक है, हम उह हूँ... हम कार्ड का उपयोग करेंगे.',\n",
              "  \"no, no, no... fine, we'll uh... we'll use the card.\"],\n",
              " ['- क्या भाषा क्या वे वहाँ बात की?', '- what language do they speak there?']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aTHZhD-WqmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1362ead-073d-46bf-a035-3922c321273e"
      },
      "source": [
        "len(train_set) #size of original train dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102322"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDT7i0fUWrfj"
      },
      "source": [
        "#removing the punctuation, unnecessary spaces and expanding words like \"can't\" to \"can not\"\n",
        "processing_dict={\"\\'re\": \"are\",\"\\'m\":\" am\", \"let\\'s\":\"let us\",\"\\'s\":\" is\",\"\\'ve\":\" have\",\"\\'ll\":\" will\",\n",
        "    \"\\'re\":\" are\", \"don\\'t\":\"do not\",\"didn't\":\"did not\",\"can\\'t\":\"can not\",\"couldn\\'t\":\"could not\",\n",
        "    \"wouldn\\'t\":\"would not\", \"doesn\\'t\":\"does not\", \"isn\\'t\":\"is not\",\"won\\'t\":\"will not\",\n",
        "    \"weren\\'t\":\"were not\",\"hadn\\'t\":\"had not\",\"aren\\'t\":\"are not\", \"hasn\\'t\":\"has not\",\n",
        "    \"wasn\\'t\":\"was not\", \"shouldn\\'t\":\"should not\",\"ain\\'t\":\"am not\",\"-\":\" \", \"(\":\" \",\")\":\" \",\"{\":\" \",\n",
        "    \"}\":\" \",\"[\":\" \", \"]\":\" \",\":\":\" \", \"\\'\":\" \",\"\\\"\":\" \",\"\\&\":\" and \", \",\":\" \",\"#\":\" \", \"/\":\" \",\"\\\\\":\" \",\n",
        "    \"♪\":\" \",\"\\=\":\" \",\"¶\":\" \",\"~\":\" \",\"  \":\" \"}\n",
        "for i in range(0,len(train_set)):\n",
        "  for j in range(0,2):\n",
        "    for (src,trg) in processing_dict.items():\n",
        "      if src in train_set[i][j]:\n",
        "        train_set[i][j]=train_set[i][j].replace(src,trg) \n",
        "    train_set[i][0]=train_set[i][0].replace(\"...\",\" \")\n",
        "    train_set[i][0]=train_set[i][0].replace(\".\",\"|\")\n",
        "    train_set[i][1]=train_set[i][1].replace(\"....\",\" \")\n",
        "    train_set[i][1]=train_set[i][1].replace(\"...\",\" \")\n",
        "    train_set[i][1]=train_set[i][1].replace(\"..\",\" \")\n",
        "with open('/content/gdrive/My Drive/Hindi to English MT/pre_processed_train.csv', 'w') as f:\n",
        "    write = csv.writer(f)\n",
        "    write.writerow(['hindi','english'])\n",
        "    write.writerows(train_set)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY0K9AGtoD6F",
        "outputId": "950b76ee-468c-45ad-93c5-9e8869845c2d"
      },
      "source": [
        "train_set[0:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['एल सालवाडोर मे जिन दोनो पक्षों ने सिविल युद्ध से वापसी ली उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।',\n",
              "  'in el salvador both sides that withdrew from their civil war took moves that had been proven to mirror a prisoner is dilemma strategy.'],\n",
              " ['मैं उनके साथ कोई लेना देना नहीं है|', 'i have nothing to do with them.'],\n",
              " [' हटाओ रिक|', 'fuck them rick.'],\n",
              " ['क्योंकि यह एक खुशियों भरी फ़िल्म है|', 'because it is a happy film.'],\n",
              " ['The thought reaching the eyes ', 'the thought reaching the eyes '],\n",
              " ['मैंने तुमे School से हटवा दिया |', 'i got you suspended.'],\n",
              " ['यह Vika एक फूल है|', 'it is a flower vika.'],\n",
              " ['पर मेरे लिए उसका यहुदी विरोधी होना उसके कार्यों को और भी प्रशंसनीय बनाता है क्योंकि उसके पास भी पक्षपात करने के वही कारण थे जो बाकी फौजियों के पास थे पर उसकी सच जानने और उसे बनाए रखने की प्रेरणा सबसे ऊपर थी',\n",
              "  'but personally for me the fact that picquart was anti semitic actually makes his actions more admirable because he had the same prejudices the same reasons to be biased as his fellow officers but his motivation to find the truth and uphold it trumped all of that.'],\n",
              " ['नहीं नहीं नहीं  ठीक है हम उह हूँ  हम कार्ड का उपयोग करेंगे|',\n",
              "  'no no no fine we will uh we will use the card.'],\n",
              " [' क्या भाषा क्या वे वहाँ बात की?', ' what language do they speak there?'],\n",
              " [' गन क्लिक करके ', ' gun clicking '],\n",
              " ['ये बिलकुल रोमांचकारी अनुभव है।', 'it is thrilling.'],\n",
              " ['तो स्मार्ट में हमारे पास लक्ष्य के अलावा मलेरिया टीका विकसित करने के हम अफ्रीकी वैज्ञानिकों को भी प्रशिक्षण दे रहे हैं क्योंकि अफ्रीका में बीमारी का बोझ काफी ज़्यादा है और आपको उन लोगों की आवश्यकता है जो सीमाओं को आगे बढ़ाना जारी रखेंगे विज्ञान में अफ्रीका में।',\n",
              "  'so in smart apart from the goal that we have to develop a malaria vaccine we are also training african scientists because the burden of disease in africa is high and you need people who will continue to push the boundaries in science in africa.'],\n",
              " ['उससे बदतर हमारे पेशे ने कानून को जटिलता का चोगा पहना दिया है।',\n",
              "  'worse our profession has shrouded law in a cloak of complexity.'],\n",
              " [' औरमैंउसे वहाँखड़े देखा थाएक ', ' and i saw her standing there '],\n",
              " ['बकवास आप क्या कर रहे हैं ', 'what the fuck are you '],\n",
              " ['क्या आपको याद है जब हमने देखा है डौग कि प्रतिमा पर impaled गद्दे?',\n",
              "  'do you remember when we saw doug is mattress impaled on that statue?'],\n",
              " ['कोई प्यार के लिए एडी?', 'no love for eddie?'],\n",
              " ['अच्छा विचार | अच्छा फोन छत पर ||', 'good idea. good call. on the roof.'],\n",
              " ['यह एक बहुत आसान हो गया होता एक सप्ताह पहले।',\n",
              "  'this would have been a lot easier a week ago.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUmseYCHwN1x"
      },
      "source": [
        "## Creating Train and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO6husMKyPhe",
        "outputId": "614f96a8-bf38-40f4-e4f9-fb78df10232b"
      },
      "source": [
        "n=len(train_set)\n",
        "train_ratio=0.80\n",
        "train_size=int(n*train_ratio)\n",
        "val_size=int(n-train_size)\n",
        "train_ds, val_ds = train_set[:train_size],train_set[train_size:]\n",
        "len(train_ds), len(val_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(81857, 20465)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67MgqAMz7dsP"
      },
      "source": [
        "with open('/content/gdrive/My Drive/Hindi to English MT/train_ds.csv', 'w') as f:\n",
        "    # using csv.writer method from CSV package\n",
        "    write = csv.writer(f)\n",
        "    write.writerows(train_ds)\n",
        "with open('/content/gdrive/My Drive/Hindi to English MT/validation_ds.csv', 'w') as f:\n",
        "    # using csv.writer method from CSV package\n",
        "    write = csv.writer(f)\n",
        "    write.writerows(val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIH6-E3Py-Fy"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NIaWIIsg5Cw"
      },
      "source": [
        "import spacy #importing spacy for english tokenizer\n",
        "eng= spacy.load(\"en\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eptp_CtK0ldG",
        "outputId": "d4885606-7207-497a-9155-0a430f2cf89b"
      },
      "source": [
        "#installing Indic NLP packages for hindi tokenizer\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!pip install Morfessor\n",
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (133/133), 149.77 MiB | 24.98 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Checking out files: 100% (28/28), done.\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 1271 (delta 50), reused 54 (delta 25), pack-reused 1178\u001b[K\n",
            "Receiving objects: 100% (1271/1271), 9.56 MiB | 13.26 MiB/s, done.\n",
            "Resolving deltas: 100% (654/654), done.\n",
            "Collecting Morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Installing collected packages: Morfessor\n",
            "Successfully installed Morfessor-2.0.6\n",
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "Successfully installed sentencepiece-0.1.95 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uzi5g-IhcnK"
      },
      "source": [
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\" # path to local git repo for Indic NLP library\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\" # path to local git repo for Indic NLP Resources"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4Uwg82Xhif2"
      },
      "source": [
        "#installing the required packages\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.data import Field, BucketIterator, Dataset, Example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnqUXctOhnMT"
      },
      "source": [
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "from indicnlp import loader\n",
        "loader.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U027dBzY9zPI"
      },
      "source": [
        "from indicnlp.tokenize import indic_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvhO-VAiidlO"
      },
      "source": [
        "# funtion to tokenize hindi text\n",
        "def hindi_tokenizer(text_in_hindi):\n",
        "  hindi_tokens=[]\n",
        "  for token in indic_tokenize.trivial_tokenize(text_in_hindi): \n",
        "    hindi_tokens.append(token)\n",
        "  return hindi_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0fQQpjoi6-M"
      },
      "source": [
        "#function to tokenize english text\n",
        "def english_tokenizer(text_in_english):\n",
        "  english_tokens=[]\n",
        "  for token in eng.tokenizer(text_in_english): \n",
        "    english_tokens.append(token.text)\n",
        "  return english_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMUIfqoVjlA4"
      },
      "source": [
        "hindi_field =Field(tokenize = hindi_tokenizer, init_token = '<sos>', eos_token = '<eos>') #field to tokeinze and load \n",
        "english_field =Field(tokenize = english_tokenizer, init_token = '<sos>', eos_token = '<eos>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ08ZSv0HEBO"
      },
      "source": [
        "fields = [('source', hindi_field), ('target', english_field)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvOn1JBQfwSV"
      },
      "source": [
        "#loading the train and validation data\n",
        "train_ds, val_ds = data.TabularDataset.splits(path = '/content/gdrive/My Drive/Hindi to English MT/',train = 'train_ds.csv', test = 'validation_ds.csv',format = 'csv',\n",
        "                            fields =[('source', hindi_field), ('target', english_field)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-yGDb0NjkVy",
        "outputId": "d760fb79-4c5c-41c4-be2b-ceba28d23570"
      },
      "source": [
        "len(train_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbfwIbrj-IKi"
      },
      "source": [
        "hindi_field.build_vocab(train_ds,max_size=20000) #hindi vocabulary\n",
        "english_field.build_vocab(train_ds,max_size=20000) #english vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDdp4cQ8728B"
      },
      "source": [
        "Defining the Encoder architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ9CIX5mg4Zl"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, layers, dropout_val):\n",
        "    #input size is equal to hindi vocabulary size and embedding_size is equal to dimensions of embeddings\n",
        "    super(Encoder, self).__init__()\n",
        "    self.dropout = nn.Dropout(dropout_val)\n",
        "    self.embedding = nn.Embedding(input_size,embedding_size)\n",
        "    #input to LSTM has dimensions equal to embedding_size and output has dimensions equal to hidden_size\n",
        "    self.lstm = nn.LSTM(embedding_size, hidden_size, layers, dropout = dropout_val)\n",
        "  \n",
        "  def forward(self,token_vec):\n",
        "    #token_vec is a vector of indices mapping a word to its index in the vocabulary\n",
        "    embedding = self.dropout(self.embedding(token_vec)) #embedding is a 3D tensor of shape (seq length, batch_size, embedding_size)\n",
        "    outputs, (hidden,cell) = self.lstm(embedding) #the embedding is passed as input to the LSTM\n",
        "    return hidden, cell #encoder output is not stored; only hidden and cell states are of concern in encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKL2XeFr78nM"
      },
      "source": [
        "Definning the Decoder architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R77v_9F4B-_R"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "   def __init__(self, input_size, embedding_size, hidden_size, output_size, layers, dropout_val):\n",
        "     # here, input_size= size of english vocabulary, embedding_size= dimensions of embedding as defined, hidden_size as defined and output_size=english vocabulary size\n",
        "     super(Decoder, self).__init__()\n",
        "     self.dropout = nn.Dropout(dropout_val)\n",
        "     self.embedding = nn.Embedding(input_size,embedding_size)\n",
        "     self.lstm = nn.LSTM(embedding_size, hidden_size, layers, dropout = dropout_val) \n",
        "     self.fc = nn.Linear(hidden_size, output_size) \n",
        "\n",
        "   def forward(self,token_vec,hidden,cell): #token_vec is one dimensional i.e. shape(token_vec) = (batch_size). \n",
        "     # However the decoder predicts one word at a time, thus the required dimensions are (1, batch_size). \n",
        "     token_vec = token_vec.unsqueeze(0) # unsqueeze adds one more dimension to token_vec\n",
        "     embedding = self.dropout(self.embedding(token_vec)) #embedding is a 3D tensor of shape (1, batch_size, embedding_size)\n",
        "     outputs, (hidden,cell) = self.lstm(embedding, (hidden,cell)) # hidden and cell states are used to determine the next word in the sequence and output is the current predicted word. \n",
        "     predictions = self.fc(outputs) # shape(outputs)= (1, batch size, hidden size), shape(predictions)= (1, batch_size, english vocabulary size)\n",
        "     predictions = predictions.squeeze(0) #remove the one extra dimension which was added using unsqueeze. \n",
        "     return predictions, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxgpaHKH8BIu"
      },
      "source": [
        "Defining the Seq2Seq class to define the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi6XfLVzJuQn"
      },
      "source": [
        "# Now we need to define a class which will define our model.\n",
        "class seq2seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(seq2seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio = 0.5): #teacher_force_ratio helps in preventing the model from overfitting and underfitting. \n",
        "        # teacher_force_ratio helps in deciding whether the next input word to the decoder will be actual/target word or the previous predicted word.\n",
        "        output_vec = torch.zeros(target.shape[0], source.shape[1], len(english_field.vocab)).to(device)\n",
        "        hidden, cell = self.encoder(source)\n",
        "        input_token = target[0]\n",
        "        for i in range(1, target.shape[0]):\n",
        "            output, hidden, cell = self.decoder(input_token, hidden, cell) # Size of output = (batch_size, eng vocabulary size)          \n",
        "            guess_val = output.argmax(1)\n",
        "            if (random.random() < teacher_force_ratio): #half of the times this will be true if teacher_force_ratio is 0.5\n",
        "              input_token = target[i]  #in this case next input to the decoder is target/actual word\n",
        "            else:  input_token= guess_val #in this case next input to the decoder is predicted word\n",
        "            output_vec[i] = output\n",
        "        return output_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "315KjUmA8MhA"
      },
      "source": [
        "Setting optimal hyperparameters for Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ea-G1QeJ4iq"
      },
      "source": [
        "#Hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "epochs =30\n",
        "epoch_loss=0.0\n",
        "layers = 2 # number of neural network layers in rnn\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder_input_size = len(hindi_field.vocab) \n",
        "decoder_input_size = len(english_field.vocab)\n",
        "output_size = len(english_field.vocab)\n",
        "hidden_size = 1024 #encoder and decoder have same hidden size\n",
        "embedding_size = 250 #encoder and decoder embedding size\n",
        "dropout = 0.5 #encoder and decoder dropout value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AskD18Gf8Yq5"
      },
      "source": [
        "Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8mKZOHGGwVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db739a4-936b-49d4-ee6c-5964ad26aebe"
      },
      "source": [
        "train_iterator, test_iterator = BucketIterator.splits((train_ds, val_ds),batch_size=batch_size,sort_within_batch=True,sort_key=lambda x: len(x.source),\n",
        "    device=device,) #creating batches of size 32 for training the model\n",
        "encoder = Encoder(encoder_input_size, embedding_size, hidden_size, layers, dropout).to(device) #passing the inputs to encoder\n",
        "decoder = Decoder(decoder_input_size, embedding_size, hidden_size, output_size,layers,dropout,).to(device) #passing the inputs to decoder\n",
        "model = seq2seq(encoder, decoder).to(device)\n",
        "pad_index = english_field.vocab.stoi['<pad>'] #finding the index of token <pad> in english vocabulary\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_index) #padding is being done to make all inputs of same length\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "step = 0\n",
        "for epoch in range(0,epochs):\n",
        "    print(f\"[Epoch {epoch} / {epochs}]\")\n",
        "    model.eval()\n",
        "    model.train()\n",
        "    for id, batch in enumerate(train_iterator):\n",
        "        input_sentence = batch.source.to(device) # get inputs and get to cuda\n",
        "        target_sentence = batch.target.to(device) # get target outputs and get to cuda\n",
        "        output = model(input_sentence, target_sentence) #forward propagation\n",
        "        output = output[1:].reshape(-1, output.shape[2]) #removing the start token from model's prediction and reshaping it to make it make it fit for input to loss function\n",
        "        target_sentence = target_sentence[1:].reshape(-1) #removing the start token from actual target translation\n",
        "        optimizer.zero_grad() \n",
        "        loss = criterion(output, target_sentence)\n",
        "        loss.backward() #backward propagation\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        optimizer.step() #gradient descent. The optimizer iterates over all parameters (tensors) to be updated and their internally stored gradients are used.\n",
        "        del target_sentence,output,input_sentence\n",
        "        step += 1\n",
        "        epoch_loss+=loss.item()\n",
        "    print(\"Epoch loss : \", loss.item())\n",
        "    # print(\"epoch loss: \", epoch_loss/len(train_iterator))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0 / 30]\n",
            "Epoch loss :  4.4644012451171875\n",
            "[Epoch 1 / 30]\n",
            "Epoch loss :  5.541215419769287\n",
            "[Epoch 2 / 30]\n",
            "Epoch loss :  5.179983615875244\n",
            "[Epoch 3 / 30]\n",
            "Epoch loss :  5.598097801208496\n",
            "[Epoch 4 / 30]\n",
            "Epoch loss :  2.7793753147125244\n",
            "[Epoch 5 / 30]\n",
            "Epoch loss :  5.716026306152344\n",
            "[Epoch 6 / 30]\n",
            "Epoch loss :  5.463747978210449\n",
            "[Epoch 7 / 30]\n",
            "Epoch loss :  2.187171459197998\n",
            "[Epoch 8 / 30]\n",
            "Epoch loss :  4.6713643074035645\n",
            "[Epoch 9 / 30]\n",
            "Epoch loss :  2.149890422821045\n",
            "[Epoch 10 / 30]\n",
            "Epoch loss :  1.905730962753296\n",
            "[Epoch 11 / 30]\n",
            "Epoch loss :  3.165379524230957\n",
            "[Epoch 12 / 30]\n",
            "Epoch loss :  2.07071852684021\n",
            "[Epoch 13 / 30]\n",
            "Epoch loss :  4.8509650230407715\n",
            "[Epoch 14 / 30]\n",
            "Epoch loss :  1.6491650342941284\n",
            "[Epoch 15 / 30]\n",
            "Epoch loss :  2.4881155490875244\n",
            "[Epoch 16 / 30]\n",
            "Epoch loss :  1.7846518754959106\n",
            "[Epoch 17 / 30]\n",
            "Epoch loss :  2.103574514389038\n",
            "[Epoch 18 / 30]\n",
            "Epoch loss :  1.5608069896697998\n",
            "[Epoch 19 / 30]\n",
            "Epoch loss :  2.937772512435913\n",
            "[Epoch 20 / 30]\n",
            "Epoch loss :  0.8639482855796814\n",
            "[Epoch 21 / 30]\n",
            "Epoch loss :  4.643232345581055\n",
            "[Epoch 22 / 30]\n",
            "Epoch loss :  1.1105540990829468\n",
            "[Epoch 23 / 30]\n",
            "Epoch loss :  0.8205732703208923\n",
            "[Epoch 24 / 30]\n",
            "Epoch loss :  1.5415409803390503\n",
            "[Epoch 25 / 30]\n",
            "Epoch loss :  1.0303735733032227\n",
            "[Epoch 26 / 30]\n",
            "Epoch loss :  1.12983238697052\n",
            "[Epoch 27 / 30]\n",
            "Epoch loss :  1.5911747217178345\n",
            "[Epoch 28 / 30]\n",
            "Epoch loss :  1.957378625869751\n",
            "[Epoch 29 / 30]\n",
            "Epoch loss :  1.7291854619979858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUzX_lk0CZ2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c694d3d3-bc1e-4b18-8339-fe4ce2a96f64"
      },
      "source": [
        "path = \"/content/gdrive/My Drive/Hindi to English MT/lstm_phase1.pth\"\n",
        "# torch.save(model,path) #saving the trained model at defined location\n",
        "# model.train()\n",
        "model = torch.load(path) #loading the model\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "seq2seq(\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(20004, 250)\n",
              "    (rnn): LSTM(250, 1024, num_layers=2, dropout=0.5)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(20004, 250)\n",
              "    (rnn): LSTM(250, 1024, num_layers=2, dropout=0.5)\n",
              "    (fc): Linear(in_features=1024, out_features=20004, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP8DEIb7-O3-"
      },
      "source": [
        "Function to translate hindi sentences(index vectors) to english sentences(index vector)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X4n-Y3kIfAP"
      },
      "source": [
        "def hin_to_eng_translation(model, hindi_field, english_field, device, hindi_num_vec, max_length=50):\n",
        "    hindi_tensor = torch.LongTensor(hindi_num_vec).unsqueeze(1).to(device)\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(hindi_tensor)\n",
        "    eng_num_vec = [english_field.vocab.stoi[\"<sos>\"]]\n",
        "    eos_idx=english_field.vocab.stoi[\"<eos>\"]\n",
        "    for _ in range(max_length):\n",
        "        curr_input = torch.LongTensor([eng_num_vec[-1]]).to(device)\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(curr_input, hidden, cell)\n",
        "            curr_output = output.argmax(1).item()\n",
        "        eng_num_vec.append(curr_output)\n",
        "        if (curr_output == eos_idx):\n",
        "            break\n",
        "    return eng_num_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQJY-N_M--Rl"
      },
      "source": [
        "## Testing the Model \n",
        "Obtaining the reference and prediction files to compute Bleu score and Meteor Score using Evaluation Script. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsVVUsiQ-NXJ"
      },
      "source": [
        "Obtaining the predicted sentences for validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHGXLM_PfLqC"
      },
      "source": [
        "file1 = open(\"/content/gdrive/My Drive/Hindi to English MT/validation_prediction.txt\",\"w\")\n",
        "csv_file = open(\"/content/gdrive/My Drive/Hindi to English MT/validation_ds.csv\",encoding='utf-8')\n",
        "rows = csv.reader(csv_file)\n",
        "for row in rows:\n",
        "    hindi_sentence = row[0]\n",
        "    hindi_sentence_token=[]\n",
        "    if type(hindi_sentence) == str:\n",
        "      for t in indic_tokenize.trivial_tokenize(hindi_sentence):\n",
        "        hindi_sentence_token.append(t)\n",
        "    else:\n",
        "        for t in hindi_sentence:\n",
        "          hindi_sentence_token.append(t)\n",
        "  \n",
        "    hindi_sentence_token.insert(0, hindi_field.init_token)\n",
        "    hindi_sentence_token.append(hindi_field.eos_token)\n",
        "    hindi_num_vec = []\n",
        "    for t in hindi_sentence_token:\n",
        "      hindi_num_vec.append(hindi_field.vocab.stoi[t])\n",
        "    eng_num_vec = hin_to_eng_translation(model, hindi_field, english_field, device, hindi_num_vec, max_length=50)\n",
        "    english_sentence_list=[]\n",
        "    for word_idx in eng_num_vec:\n",
        "      english_sentence_list.append(english_field.vocab.itos[word_idx])\n",
        "    english_sentence_list.pop(0)\n",
        "    if (len(english_sentence_list)>2):\n",
        "      english_sentence=str(english_sentence_list[0][0].upper()) + str(english_sentence_list[0][1:]) +\" \"\n",
        "      for string in english_sentence_list[1:-3]:\n",
        "           english_sentence+=string+' '\n",
        "      english_sentence+= str(english_sentence_list[-3])+str(english_sentence_list[-2])\n",
        "    else: \n",
        "      english_sentence=str(english_sentence_list[0][0].upper()) + str(english_sentence_list[0][1:])\n",
        "    file1.write(english_sentence + \"\\n\")\n",
        "  \n",
        "file1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP0scsSD-veO"
      },
      "source": [
        "Saving the refernce sentences for validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvZ_2KS6R-6A"
      },
      "source": [
        "csv_file = open(\"/content/gdrive/My Drive/Hindi to English MT/validation_ds.csv\",encoding='utf-8')\n",
        "rows = csv.reader(csv_file)\n",
        "file = open(\"/content/gdrive/My Drive/Hindi to English MT/validation_english.txt\",\"w\")\n",
        "for row in rows:\n",
        "    file.write(row[1]+\"\\n\")\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm9H6Xuc_YEa"
      },
      "source": [
        "### Obtaining translation for hindistatements.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7OHLqOmR_j9"
      },
      "source": [
        "file1 = open(\"/content/gdrive/My Drive/Hindi to English MT/predicted_text.txt\",\"w\")\n",
        "csv_file = open(\"/content/gdrive/My Drive/Hindi to English MT/hindistatements.csv\",encoding='utf-8')\n",
        "rows = csv.reader(csv_file)\n",
        "i=0\n",
        "for row in rows:\n",
        "    if (i==0): \n",
        "      i+=1\n",
        "      continue\n",
        "    hindi_sentence = row[2]\n",
        "    hindi_sentence_token=[]\n",
        "    if type(hindi_sentence) == str:\n",
        "      for t in indic_tokenize.trivial_tokenize(hindi_sentence):\n",
        "        hindi_sentence_token.append(t)\n",
        "    else:\n",
        "        for t in hindi_sentence:\n",
        "          hindi_sentence_token.append(t) \n",
        "    hindi_sentence_token.insert(0, hindi_field.init_token)\n",
        "    hindi_sentence_token.append(hindi_field.eos_token)\n",
        "    hindi_num_vec = []\n",
        "    for t in hindi_sentence_token:\n",
        "      hindi_num_vec.append(hindi_field.vocab.stoi[t])\n",
        "    eng_num_vec = hin_to_eng_translation(model, hindi_field, english_field, device, hindi_num_vec, max_length=50)\n",
        "    english_sentence_list=[]\n",
        "    for word_idx in eng_num_vec:\n",
        "      english_sentence_list.append(english_field.vocab.itos[word_idx])\n",
        "    english_sentence_list.pop(0)\n",
        "    if (len(english_sentence_list)>2):\n",
        "      english_sentence=str(english_sentence_list[0][0].upper()) + str(english_sentence_list[0][1:]) +\" \"\n",
        "      for string in english_sentence_list[1:-3]:\n",
        "           english_sentence+=string+' '\n",
        "      english_sentence+= str(english_sentence_list[-3])+str(english_sentence_list[-2])\n",
        "    else: \n",
        "      english_sentence=str(english_sentence_list[0][0].upper()) + str(english_sentence_list[0][1:])\n",
        "    file1.write(english_sentence + \"\\n\")\n",
        "file1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l28Q66QX1RcL"
      },
      "source": [
        "## Refrences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XKCl_kG3fMQ"
      },
      "source": [
        "1. https://arxiv.org/abs/1409.3215\n",
        "2. https://medium.com/analytics-vidhya/neural-machine-translation-for-hindi-english-sequence-to-sequence-learning-1298655e334a\n"
      ]
    }
  ]
}
