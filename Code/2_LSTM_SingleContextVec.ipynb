{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assign1_phase3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1VPqyMPen34"
      },
      "source": [
        "# 1.Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pyUyWyterCC"
      },
      "source": [
        "This notebook contains the sequence to sequence model for Hindi to English Neural Machine Translation using Pytorch. For encoder and decoder, a uni-directional LSTM(Long Short-Term Memory) with 2 neural layers is used. The source and target language sentences are appended with start of sequence (\\<sos\\>) and end of sequence (\\<eos\\>) tokens. IndicNLP is used for tokenization of Hindi and English sentences. Cross Entropy Loss Function is used for computation of loss and to update the parameters of the model.\n",
        "The notebook is divided into the following sections:\n",
        "1. Introduction\n",
        "2. Installing the required packages\n",
        "3. Pre-processing data\n",
        "4. Building the Vocabulary\n",
        "5. Model Architecture\n",
        "6. Training the Model\n",
        "7. Testing the Model\n",
        "8. Generating Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhteQdgti9jf"
      },
      "source": [
        "# 2. Installing the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK2kj6qucJL3"
      },
      "source": [
        "import csv\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "futxYnj4q_wY",
        "outputId": "d75dea88-dc3e-46a8-f694-3cc3ff83db08"
      },
      "source": [
        "#installing Indic NLP packages for hindi and english tokenizer\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'indic_nlp_resources' already exists and is not an empty directory.\n",
            "fatal: destination path 'indic_nlp_library' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ08lNgNrCgd"
      },
      "source": [
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\" # path to local git repo for Indic NLP library\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\" # path to local git repo for Indic NLP Resources"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl2wzBelrI3O"
      },
      "source": [
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "from indicnlp import loader\n",
        "loader.load()\n",
        "from indicnlp.tokenize import indic_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLth0pEXdtfS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a85e9da-c91c-4a16-c308-ff1364f7b88f"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nKE4hMlWXVQ"
      },
      "source": [
        "# 3. Pre-processing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6609FXusr0wf"
      },
      "source": [
        "In this section, the training data is prepared. All the English letters are converted to lower case and clitics are also converted to their original words, for example, \"you're\" is replaced with \"you are\". Similarly other clitics like 're, 'm, 've, 's, 'll, n't are replaced with are, am, have, is, will, not respectively. Punctuation marks and symbols like -, (, ), {, }, [, ], :, ',\\\", \\&, , , #, /, \\\\, ♪, \\=, ¶, ~ are also removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRg3kwmCWdCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8bb0cd-482a-440d-8396-47557cad5dd9"
      },
      "source": [
        "train_set=[] #list to store pair of Hindi and English sentences\n",
        "i=0\n",
        "with open('train.csv', 'r') as f: #reading the train.csv file\n",
        "    csv_reader = csv.reader(f, delimiter=',')\n",
        "    for row in csv_reader:\n",
        "      if (i==0): # To skip the column names from getting stored in the train data list.\n",
        "        i+=1\n",
        "        continue\n",
        "      train_set.append([row[1],row[2].lower()]) # lower casing the english sentences while storing them in list\n",
        "train_set[0:10] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध से वापसी ली, उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।',\n",
              "  \"in el salvador, both sides that withdrew from their civil war took moves that had been proven to mirror a prisoner's dilemma strategy.\"],\n",
              " ['मैं उनके साथ कोई लेना देना नहीं है.', 'i have nothing to do with them.'],\n",
              " ['-हटाओ रिक.', 'fuck them, rick.'],\n",
              " ['क्योंकि यह एक खुशियों भरी फ़िल्म है.', \"because it's a happy film.\"],\n",
              " ['The thought reaching the eyes...', 'the thought reaching the eyes...'],\n",
              " ['मैंने तुमे School से हटवा दिया .', 'i got you suspended.'],\n",
              " ['यह Vika, एक फूल है.', \"it's a flower, vika.\"],\n",
              " ['पर मेरे लिए उसका यहुदी विरोधी होना उसके कार्यों को और भी प्रशंसनीय बनाता है क्योंकि उसके पास भी पक्षपात करने के वही कारण थे जो बाकी फौजियों के पास थे पर उसकी सच जानने और उसे बनाए रखने की प्रेरणा सबसे ऊपर थी',\n",
              "  'but personally, for me, the fact that picquart was anti-semitic actually makes his actions more admirable, because he had the same prejudices, the same reasons to be biased as his fellow officers, but his motivation to find the truth and uphold it trumped all of that.'],\n",
              " ['नहीं, नहीं, नहीं... ठीक है, हम उह हूँ... हम कार्ड का उपयोग करेंगे.',\n",
              "  \"no, no, no... fine, we'll uh... we'll use the card.\"],\n",
              " ['- क्या भाषा क्या वे वहाँ बात की?', '- what language do they speak there?']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aTHZhD-WqmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a52604a-4ac5-46d1-bc05-e30eb256db08"
      },
      "source": [
        "len(train_set) #size of original train dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102322"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDT7i0fUWrfj"
      },
      "source": [
        "#removing the punctuation, unnecessary spaces and expanding words like \"can't\" to \"can not\"\n",
        "processing_dict={\"\\'re\": \"are\",\"\\'m\":\" am\", \"let\\'s\":\"let us\",\"\\'s\":\" is\",\"\\'ve\":\" have\",\"\\'ll\":\" will\",\n",
        "    \"\\'re\":\" are\", \"don\\'t\":\"do not\",\"didn't\":\"did not\",\"can\\'t\":\"can not\",\"couldn\\'t\":\"could not\",\n",
        "    \"wouldn\\'t\":\"would not\", \"doesn\\'t\":\"does not\", \"isn\\'t\":\"is not\",\"won\\'t\":\"will not\",\n",
        "    \"weren\\'t\":\"were not\",\"hadn\\'t\":\"had not\",\"aren\\'t\":\"are not\", \"hasn\\'t\":\"has not\",\n",
        "    \"wasn\\'t\":\"was not\", \"shouldn\\'t\":\"should not\",\"ain\\'t\":\"am not\",\"-\":\" \", \"(\":\" \",\")\":\" \",\"{\":\" \",\n",
        "    \"}\":\" \",\"[\":\" \", \"]\":\" \",\":\":\" \", \"\\'\":\" \",\"\\\"\":\" \",\"\\&\":\" and \", \",\":\" \",\"#\":\" \", \"/\":\" \",\"\\\\\":\" \",\n",
        "    \"♪\":\" \",\"\\=\":\" \",\"¶\":\" \",\"~\":\" \",\"  \":\" \"}\n",
        "for i in range(0,len(train_set)):\n",
        "  for j in range(0,2):\n",
        "    for (src,trg) in processing_dict.items():\n",
        "      if src in train_set[i][j]:\n",
        "        train_set[i][j]=train_set[i][j].replace(src,trg) \n",
        "    train_set[i][0]=train_set[i][0].replace(\"...\",\" \")\n",
        "    train_set[i][0]=train_set[i][0].replace(\".\",\"|\")\n",
        "    train_set[i][1]=train_set[i][1].replace(\"....\",\" \")\n",
        "    train_set[i][1]=train_set[i][1].replace(\"...\",\" \")\n",
        "    train_set[i][1]=train_set[i][1].replace(\"..\",\" \")\n",
        "with open('pre_processed_train.csv', 'w') as f:\n",
        "    write = csv.writer(f)\n",
        "    write.writerow(['hindi','english'])\n",
        "    write.writerows(train_set)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY0K9AGtoD6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb323146-1836-4475-8a43-8a505029f7d2"
      },
      "source": [
        "train_set[0:20] # training data after pre-processing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['एल सालवाडोर मे जिन दोनो पक्षों ने सिविल युद्ध से वापसी ली उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।',\n",
              "  'in el salvador both sides that withdrew from their civil war took moves that had been proven to mirror a prisoner is dilemma strategy.'],\n",
              " ['मैं उनके साथ कोई लेना देना नहीं है|', 'i have nothing to do with them.'],\n",
              " [' हटाओ रिक|', 'fuck them rick.'],\n",
              " ['क्योंकि यह एक खुशियों भरी फ़िल्म है|', 'because it is a happy film.'],\n",
              " ['The thought reaching the eyes ', 'the thought reaching the eyes '],\n",
              " ['मैंने तुमे School से हटवा दिया |', 'i got you suspended.'],\n",
              " ['यह Vika एक फूल है|', 'it is a flower vika.'],\n",
              " ['पर मेरे लिए उसका यहुदी विरोधी होना उसके कार्यों को और भी प्रशंसनीय बनाता है क्योंकि उसके पास भी पक्षपात करने के वही कारण थे जो बाकी फौजियों के पास थे पर उसकी सच जानने और उसे बनाए रखने की प्रेरणा सबसे ऊपर थी',\n",
              "  'but personally for me the fact that picquart was anti semitic actually makes his actions more admirable because he had the same prejudices the same reasons to be biased as his fellow officers but his motivation to find the truth and uphold it trumped all of that.'],\n",
              " ['नहीं नहीं नहीं  ठीक है हम उह हूँ  हम कार्ड का उपयोग करेंगे|',\n",
              "  'no no no fine we will uh we will use the card.'],\n",
              " [' क्या भाषा क्या वे वहाँ बात की?', ' what language do they speak there?'],\n",
              " [' गन क्लिक करके ', ' gun clicking '],\n",
              " ['ये बिलकुल रोमांचकारी अनुभव है।', 'it is thrilling.'],\n",
              " ['तो स्मार्ट में हमारे पास लक्ष्य के अलावा मलेरिया टीका विकसित करने के हम अफ्रीकी वैज्ञानिकों को भी प्रशिक्षण दे रहे हैं क्योंकि अफ्रीका में बीमारी का बोझ काफी ज़्यादा है और आपको उन लोगों की आवश्यकता है जो सीमाओं को आगे बढ़ाना जारी रखेंगे विज्ञान में अफ्रीका में।',\n",
              "  'so in smart apart from the goal that we have to develop a malaria vaccine we are also training african scientists because the burden of disease in africa is high and you need people who will continue to push the boundaries in science in africa.'],\n",
              " ['उससे बदतर हमारे पेशे ने कानून को जटिलता का चोगा पहना दिया है।',\n",
              "  'worse our profession has shrouded law in a cloak of complexity.'],\n",
              " [' औरमैंउसे वहाँखड़े देखा थाएक ', ' and i saw her standing there '],\n",
              " ['बकवास आप क्या कर रहे हैं ', 'what the fuck are you '],\n",
              " ['क्या आपको याद है जब हमने देखा है डौग कि प्रतिमा पर impaled गद्दे?',\n",
              "  'do you remember when we saw doug is mattress impaled on that statue?'],\n",
              " ['कोई प्यार के लिए एडी?', 'no love for eddie?'],\n",
              " ['अच्छा विचार | अच्छा फोन छत पर ||', 'good idea. good call. on the roof.'],\n",
              " ['यह एक बहुत आसान हो गया होता एक सप्ताह पहले।',\n",
              "  'this would have been a lot easier a week ago.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUmseYCHwN1x"
      },
      "source": [
        "### Creating Train and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO6husMKyPhe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6e0576-87ab-438d-ef9b-55d198cd1f15"
      },
      "source": [
        "n=len(train_set)\n",
        "train_ratio=0.80 # 80:20 ratio is used for train and validation/test data\n",
        "train_size=int(n*train_ratio)\n",
        "val_size=int(n-train_size)\n",
        "train_ds, val_ds = train_set[:train_size],train_set[train_size:]\n",
        "len(train_ds), len(val_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(81857, 20465)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67MgqAMz7dsP"
      },
      "source": [
        "# saving the train and validation data in csv file\n",
        "\n",
        "with open('train_ds.csv', 'w') as f:\n",
        "    # using csv.writer method from CSV package\n",
        "    write = csv.writer(f)\n",
        "    write.writerows(train_ds)\n",
        "\n",
        "\n",
        "with open('validation_ds.csv', 'w') as f:\n",
        "    # using csv.writer method from CSV package\n",
        "    write = csv.writer(f)\n",
        "    write.writerows(val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UenjvmTJGWci"
      },
      "source": [
        "# 4.Building the Vocabulary\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvhO-VAiidlO"
      },
      "source": [
        "# funtion to tokenize hindi text\n",
        "def hindi_tokenizer(text_in_hindi):\n",
        "  hindi_tokens=[]\n",
        "  for token in indic_tokenize.trivial_tokenize(text_in_hindi): # trivial_tokenize of indicNLP is used for tokenization\n",
        "    hindi_tokens.append(token)\n",
        "  return hindi_tokens # tokens of a sentence are returned as list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0fQQpjoi6-M"
      },
      "source": [
        "# function to tokenize english text\n",
        "def english_tokenizer(text_in_english):\n",
        "  english_tokens=[]\n",
        "  for token in indic_tokenize.trivial_tokenize(text_in_english): # trivial_tokenize of indicNLP is used for tokenization\n",
        "    english_tokens.append(token)\n",
        "  return english_tokens # tokens of a sentence are returned as list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUlxrP46Egqr"
      },
      "source": [
        "sos_token='<sos>' # start of sequence token; appended at start of sentence\n",
        "eos_token='<eos>' # end of sequence token; appended at end of sentence\n",
        "unk_token='<unk>' # unknown token; used to represent a word if that word is not found in the dictionary\n",
        "pad_token='<pad>' # token for padding; used to make all sentences of equal length in a batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDpAf8YdBMv5"
      },
      "source": [
        "# dictionary to keep count of occurrence of each English word\n",
        "E_wordCount={} \n",
        "\n",
        "# dictioanry to find the index for a word in English\n",
        "E_word2index={sos_token:0, eos_token:1, unk_token:2, pad_token:3}\n",
        "\n",
        "# dictionary to find the English word for a particular index\n",
        "E_index2word={0:sos_token, 1:eos_token, 2:unk_token, 3:pad_token} \n",
        "\n",
        "# dictionary to keep count of occurrence of each Hindi word\n",
        "H_wordCount={} \n",
        "\n",
        "# dictioanry to find the index for a word in Hindi\n",
        "H_word2index={sos_token:0, eos_token:1, unk_token:2, pad_token:3}\n",
        "\n",
        "# dictionary to find the Hindi word for a particular index\n",
        "H_index2word={0:sos_token, 1:eos_token, 2:unk_token, 3:pad_token}\n",
        "\n",
        "E_count=4 # keeps count of number of words so far in English dictionary\n",
        "H_count=4 # keeps count of number of words so far in Hindi dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlIueCw2x_vr"
      },
      "source": [
        "# function to add a word in English dictionary\n",
        "def E_updateDict(eng_sentence):\n",
        "  global E_count\n",
        "  tokens= english_tokenizer(eng_sentence) #generating tokens for the given sentence\n",
        "  for token in tokens:\n",
        "    if token not in E_word2index.keys(): # check if the token already exists in English dictionary\n",
        "      # if the token is not present in English dictionary then add it to word2index and index2word English dictionary\n",
        "      E_word2index[token]= E_count\n",
        "      E_index2word[E_count]=token\n",
        "      E_wordCount[token]=1\n",
        "      E_count+=1 # increasing the count of words in English vocabulary\n",
        "    else:\n",
        "      E_wordCount[token]+=1 # if the token exists in dictionary then simply increase it's count of occurrence\n",
        "\n",
        "# function to add a word in Hindi dictionary\n",
        "def H_updateDict(hindi_sentence):\n",
        "  global H_count\n",
        "  tokens= hindi_tokenizer(hindi_sentence) #generating tokens for the given sentence\n",
        "  for token in tokens:\n",
        "    if token not in H_word2index.keys(): # check if the token already exists in Hindi dictionary\n",
        "      # if the token is not present in Hindi dictionary then add it to word2index and index2word Hindi dictionary\n",
        "      H_word2index[token]= H_count\n",
        "      H_index2word[H_count]=token\n",
        "      H_wordCount[token]=1\n",
        "      H_count+=1 # increasing the count of words in Hindi vocabulary\n",
        "    else:\n",
        "      H_wordCount[token]+=1 # if the token exists in dictionary then simply increase it's count of occurrence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wATPy_6XFPfY"
      },
      "source": [
        "# reading the training pairs to create hindi and english vocabulary\n",
        "for pair in train_ds:\n",
        "  H_updateDict(pair[0]) # updating hindi vocabulary\n",
        "  E_updateDict(pair[1]) # updating english vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_jXy7mJKH3T",
        "outputId": "dde252c4-baa0-4d7e-b280-80fb5c9249a3"
      },
      "source": [
        "# number of words in hindi and english vocabulary\n",
        "print(H_count, E_count) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41006 28207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MS-pSY99XG5"
      },
      "source": [
        "# 5. Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDdp4cQ8728B"
      },
      "source": [
        "Defining the Encoder architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ9CIX5mg4Zl"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, layers, dropout_val):\n",
        "    #input size is equal to hindi vocabulary size and embedding_size is equal to dimensions of embeddings\n",
        "    super(Encoder, self).__init__()\n",
        "    self.dropout = nn.Dropout(dropout_val)\n",
        "    self.embedding = nn.Embedding(input_size,embedding_size)\n",
        "    #input to LSTM has dimensions equal to embedding_size and output has dimensions equal to hidden_size\n",
        "    self.lstm = nn.LSTM(embedding_size, hidden_size, layers, dropout = dropout_val)\n",
        "  \n",
        "  def forward(self,token_vec):\n",
        "    #token_vec is a vector of indices mapping a word to its index in the vocabulary\n",
        "    embedding = self.dropout(self.embedding(token_vec)) #embedding is a 3D tensor of shape (seq length, batch_size, embedding_size)\n",
        "    outputs, (hidden,cell) = self.lstm(embedding) #the embedding is passed as input to the LSTM\n",
        "    return hidden, cell #encoder output is not stored; only hidden and cell states are of concern in encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKL2XeFr78nM"
      },
      "source": [
        "Definning the Decoder architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R77v_9F4B-_R"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "   def __init__(self, input_size, embedding_size, hidden_size, output_size, layers, dropout_val):\n",
        "     # here, input_size= size of english vocabulary, embedding_size= dimensions of embedding as defined, hidden_size as defined and output_size=english vocabulary size\n",
        "     super(Decoder, self).__init__()\n",
        "     self.dropout = nn.Dropout(dropout_val)\n",
        "     self.embedding = nn.Embedding(input_size,embedding_size)\n",
        "     self.lstm = nn.LSTM(embedding_size, hidden_size, layers, dropout = dropout_val) \n",
        "     self.fc = nn.Linear(hidden_size, output_size) \n",
        "\n",
        "   def forward(self,token_vec,hidden,cell): #token_vec is one dimensional i.e. shape(token_vec) = (batch_size). \n",
        "     # However the decoder predicts one word at a time, thus the required dimensions are (1, batch_size). \n",
        "     token_vec = token_vec.unsqueeze(0) # unsqueeze adds one more dimension to token_vec\n",
        "     embedding = self.dropout(self.embedding(token_vec)) # embedding is a 3D tensor of shape (1, batch_size, embedding_size)\n",
        "     outputs, (hidden,cell) = self.lstm(embedding, (hidden,cell)) # hidden and cell states are used to determine the next word in the sequence and output is the current predicted word. \n",
        "     predictions = self.fc(outputs) # shape(outputs)= (1, batch size, hidden size), shape(predictions)= (1, batch_size, english vocabulary size)\n",
        "     predictions = predictions.squeeze(0) # remove the one extra dimension which was added using unsqueeze. \n",
        "     return predictions, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxgpaHKH8BIu"
      },
      "source": [
        "Defining the Seq2Seq class to define the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi6XfLVzJuQn"
      },
      "source": [
        "# Now we need to define a class which will define our model.\n",
        "class seq2seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(seq2seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio = 0.5): #teacher_force_ratio helps in preventing the model from overfitting and underfitting. \n",
        "        # teacher_force_ratio helps in deciding whether the next input word to the decoder will be actual/target word or the previous predicted word.\n",
        "        output_vec = torch.zeros(target.shape[0], source.shape[1], E_count).to(device)\n",
        "        hidden, cell = self.encoder(source)\n",
        "        input_token = target[0]\n",
        "        for i in range(1, target.shape[0]):\n",
        "            output, hidden, cell = self.decoder(input_token, hidden, cell) # Size of output = (batch_size, eng vocabulary size)          \n",
        "            guess_val = output.argmax(1)\n",
        "            if (random.random() < teacher_force_ratio): #half of the times this will be true if teacher_force_ratio is 0.5\n",
        "              input_token = target[i]  #in this case next input to the decoder is target/actual word\n",
        "            else:  input_token= guess_val #in this case next input to the decoder is predicted word\n",
        "            output_vec[i] = output\n",
        "        return output_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLcoQOYF-P5P"
      },
      "source": [
        "# 6. Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "315KjUmA8MhA"
      },
      "source": [
        "Setting optimal hyperparameters for Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ea-G1QeJ4iq"
      },
      "source": [
        "#Hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "epochs =30\n",
        "epoch_loss=0.0 # training loss in each epoch\n",
        "layers = 2 # number of neural network layers in rnn\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder_input_size = H_count \n",
        "decoder_input_size = E_count\n",
        "output_size = E_count\n",
        "hidden_size = 512 # encoder and decoder have same hidden size\n",
        "embedding_size = 250 # encoder and decoder embedding size\n",
        "dropout = 0.5 # encoder and decoder dropout value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AskD18Gf8Yq5"
      },
      "source": [
        "**Preparing data for training the Model:** First, the data is sorted according to the length of Hindi sentences and then index vectors for these sentences are found using H_sentenceToTensor and E_sentenceToTensor functions. The logic behind sorting the training data is that in one batch we want sentences of similar lengths, so sorting helps us achieve that and padding is performed whereever necessary. To create batches of same length, I calculated the maximum length of sentence in a batch and stored this value in a dictionary with key as batch_id. After obtaining the maximum length for each batch, \"\\<pad\\>\" token was appended to the sentences whose length was less than the maximum length of sentence in that batch. After that, Dataloader is used to create batches of the required batch size. Each batch will have sentences of same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUuKTQ_rpMbe"
      },
      "source": [
        "# sorting the training data according to length of hindi sentences\n",
        "train_ds.sort(key= lambda x: len(x[0])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZmj-IzHS4qx"
      },
      "source": [
        "# finding the maximum length of sentence in a batch\n",
        "max_length_train={} # stores maximum length of sentences in training data\n",
        "batch_id=1\n",
        "# computing maximum length for each batch of training data\n",
        "for i in range(0,len(train_ds),batch_size):\n",
        "  max_len=0 \n",
        "  for pair in train_ds[i:i+batch_size]:\n",
        "    E_maxlength=0\n",
        "    H_maxlength=0\n",
        "    for token in indic_tokenize.trivial_tokenize(pair[0]):\n",
        "      H_maxlength+=1\n",
        "    for token in indic_tokenize.trivial_tokenize(pair[1]):\n",
        "      E_maxlength+=1\n",
        "    max_len=max(max_len,E_maxlength, H_maxlength )\n",
        "  max_length_train[batch_id]=max_len+2\n",
        "  batch_id+=1\n",
        "\n",
        "max_length_test={} # stores maximum length of sentences in test/validation data\n",
        "batch_id=1\n",
        "# computing maximum length for each batch of validation data\n",
        "for i in range(0,len(val_ds),batch_size):\n",
        "  max_len=0\n",
        "  for pair in val_ds[i:i+batch_size]:\n",
        "    E_maxlength=0\n",
        "    H_maxlength=0\n",
        "    for token in indic_tokenize.trivial_tokenize(pair[0]):\n",
        "      H_maxlength+=1\n",
        "    for token in indic_tokenize.trivial_tokenize(pair[1]):\n",
        "      E_maxlength+=1\n",
        "    max_len=max(max_len,E_maxlength, H_maxlength )\n",
        "  max_length_test[batch_id]=max_len+2\n",
        "  batch_id+=1   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvWA6jNEqF_W"
      },
      "source": [
        "# H_sentenceToTensor function takes a sentence, maximum length as argument and returns a tensor of indices with padding done, if required.\n",
        "def H_sentenceToTensor(sentence,max_length):\n",
        "  # append start of sequence token at beginning\n",
        "  src_index=[H_word2index['<sos>']] \n",
        "  for token in hindi_tokenizer(sentence):\n",
        "     # if the word in not present in dictionary then index corresponding to unknown token '<unk>' i.e. 2 is used\n",
        "    src_index.append(H_word2index.get(token, 2))\n",
        "  # append end of sequence token\n",
        "  src_index.append(H_word2index['<eos>'])\n",
        "  # check if length of sentence is less than maximum length, if yes, then append <pad> token\n",
        "  if(len(src_index)<max_length):\n",
        "    while(len(src_index)!=max_length):\n",
        "      src_index.append(H_word2index['<pad>'])\n",
        "  return torch.Tensor(src_index) # returning tensor of indices with length equal to max_length\n",
        "\n",
        "# H_sentenceToTensor function takes a sentence, maximum length as argument and returns a tensor of indices with padding done, if required.\n",
        "def E_sentenceToTensor(sentence,max_length):\n",
        "  # append start of sequence token at beginning\n",
        "  trg_index=[E_word2index['<sos>']]\n",
        "  for token in english_tokenizer(sentence):\n",
        "    # if the word in not present in dictionary then index corresponding to unknown token '<unk>' i.e. 2 is used\n",
        "    trg_index.append(E_word2index.get(token,2))\n",
        "  # append end of sequence token\n",
        "  trg_index.append(E_word2index['<eos>'])\n",
        "  # check if length of sentence is less than maximum length, if yes, then append <pad> token\n",
        "  if(len(trg_index)<max_length): \n",
        "    while(len(trg_index)!=max_length):\n",
        "      trg_index.append(E_word2index['<pad>'])\n",
        "  return torch.Tensor(trg_index) # returning tensor of indices with length equal to max_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxSftaT2fVD-"
      },
      "source": [
        "train_tensor=[] # stores tensor of indexes of training data\n",
        "test_tensor=[] # stores tensor of indexes of validation/test data\n",
        "\n",
        "# finding tensor of indexes of training data\n",
        "batch_id=1\n",
        "for i in range(0,len(train_ds),batch_size):\n",
        "  max_len=max_length_train[batch_id]\n",
        "  for pair in train_ds[i:i+batch_size]:\n",
        "    source_tensor=H_sentenceToTensor(pair[0],max_len)\n",
        "    target_tensor=E_sentenceToTensor(pair[1],max_len)\n",
        "    train_tensor.append([source_tensor, target_tensor])\n",
        "  batch_id+=1\n",
        "\n",
        "# finding tensor of indexes of validation/test data\n",
        "batch_id=1\n",
        "for i in range(0,len(val_ds),batch_size):\n",
        "  max_len=max_length_test[batch_id]\n",
        "  for pair in val_ds[i:i+batch_size]:\n",
        "    source_tensor=H_sentenceToTensor(pair[0],max_len)\n",
        "    target_tensor=E_sentenceToTensor(pair[1],max_len)\n",
        "    test_tensor.append([source_tensor, target_tensor])\n",
        "  batch_id+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB0iFahMYf68",
        "outputId": "7cc05c0d-817f-45aa-b6f6-cad86760d1ed"
      },
      "source": [
        "print(len(train_tensor))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "81857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3biKde5Uoqf"
      },
      "source": [
        "# finding train and test iterator using data loader\n",
        "# shuffle=false is used so that data remains sorted in batches\n",
        "train_iterator = DataLoader(train_tensor, batch_size=batch_size,shuffle=False) \n",
        "test_iterator = DataLoader(test_tensor, batch_size=batch_size,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWdhymbD0aGu"
      },
      "source": [
        "# function to evaluate the validation loss in each epoch\n",
        "def evaluate(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for i, (x,y) in enumerate(iterator):\n",
        "      input_sentence = x.long().to(device) \n",
        "      target_sentence = y.long().to(device)\n",
        "      # input_sentence and target_sentence have shape = (batch_size, maximum length) but we need shape to be (maximum length, batch_size ) so they are transposed\n",
        "      input_sentence=torch.transpose(input_sentence, 0, 1)\n",
        "      target_sentence=torch.transpose(target_sentence, 0, 1)\n",
        "      output = model(input_sentence, target_sentence, 0) #turn off teacher forcing\n",
        "      output_dim = output.shape[2]\n",
        "      # output.shape() = (target len, batch size, output dim)\n",
        "      output = output[1:].reshape(-1, output_dim)\n",
        "      target_sentence = target_sentence[1:].reshape(-1)\n",
        "      loss = criterion(output, target_sentence)\n",
        "      epoch_loss += loss.item()\n",
        "      del target_sentence,output,input_sentence\n",
        "  return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9P9aUnZFBux"
      },
      "source": [
        "path = \"phase3.pth\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh_LXPFTfD1H",
        "outputId": "0859bb9e-4dd7-45bf-cc4a-9bd45cafa246"
      },
      "source": [
        "encoder = Encoder(encoder_input_size, embedding_size, hidden_size, layers, dropout).to(device) #passing the inputs to encoder\n",
        "decoder = Decoder(decoder_input_size, embedding_size, hidden_size, output_size,layers,dropout,).to(device) #passing the inputs to decoder\n",
        "model = seq2seq(encoder, decoder).to(device)\n",
        "pad_index = E_word2index['<pad>'] #finding the index of token <pad> in english vocabulary\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_index) #padding token is being ignored while loss computation because we don't want to pay price for <pad> token\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate) # AdamW optimizer is used \n",
        "step = 0\n",
        "for epoch in range(0,epochs):\n",
        "    print(f\"[Epoch {epoch} / {epochs}]\")\n",
        "    model.eval()\n",
        "    model.train()\n",
        "    i=0\n",
        "    for id, (x,y) in enumerate(train_iterator):   # iterating over batches of train_iterator\n",
        "      input_sentence = x.long().to(device)\n",
        "      target_sentence = y.long().to(device)\n",
        "\n",
        "      # input_sentence and target_sentence have shape = (batch_size, maximum length) but we need shape to be (maximum length, batch_size ) so they are transposed\n",
        "      input_sentence=torch.transpose(input_sentence, 0, 1)\n",
        "      target_sentence=torch.transpose(target_sentence, 0, 1)\n",
        "      \n",
        "      output = model(input_sentence, target_sentence) #forward propagation\n",
        "      output = output[1:].reshape(-1, output.shape[2]) #removing the start token from model's prediction and reshaping it to make it make it fit for input to loss function\n",
        "      \n",
        "      target_sentence = target_sentence[1:].reshape(-1) #removing the start token from actual target translation\n",
        "      optimizer.zero_grad() \n",
        "      loss = criterion(output, target_sentence)\n",
        "      loss.backward() #backward propagation\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # clipping the gradients to keep them in reasonable range\n",
        "      optimizer.step() #gradient descent. The optimizer iterates over all parameters (tensors) to be updated and their internally stored gradients are used.\n",
        "      del target_sentence,output,input_sentence\n",
        "      step += 1\n",
        "      epoch_loss+=loss.item()\n",
        "    if(epoch%5==0): # saving the model in every 5 iterations\n",
        "      torch.save(model,path)\n",
        "    val_loss=evaluate(model, test_iterator, criterion)\n",
        "    print(\"Train loss : \", loss.item())\n",
        "    print(\"Validation loss : \", val_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0 / 30]\n",
            "Train loss :  7.253509521484375\n",
            "Validation loss :  6.6556614227592945\n",
            "[Epoch 1 / 30]\n",
            "Train loss :  6.753749370574951\n",
            "Validation loss :  6.2911082722246645\n",
            "[Epoch 2 / 30]\n",
            "Train loss :  6.3020243644714355\n",
            "Validation loss :  6.189002197980881\n",
            "[Epoch 3 / 30]\n",
            "Train loss :  5.885638236999512\n",
            "Validation loss :  6.111462603509426\n",
            "[Epoch 4 / 30]\n",
            "Train loss :  5.538120269775391\n",
            "Validation loss :  6.056318908184767\n",
            "[Epoch 5 / 30]\n",
            "Train loss :  5.267170429229736\n",
            "Validation loss :  6.093760447204113\n",
            "[Epoch 6 / 30]\n",
            "Train loss :  5.208654880523682\n",
            "Validation loss :  6.030258818715811\n",
            "[Epoch 7 / 30]\n",
            "Train loss :  4.856709957122803\n",
            "Validation loss :  6.0159385114908215\n",
            "[Epoch 8 / 30]\n",
            "Train loss :  4.896815299987793\n",
            "Validation loss :  6.125546030700207\n",
            "[Epoch 9 / 30]\n",
            "Train loss :  4.800422191619873\n",
            "Validation loss :  6.063683351874351\n",
            "[Epoch 10 / 30]\n",
            "Train loss :  4.802103519439697\n",
            "Validation loss :  6.140982373803854\n",
            "[Epoch 11 / 30]\n",
            "Train loss :  4.603856563568115\n",
            "Validation loss :  6.152439747005701\n",
            "[Epoch 12 / 30]\n",
            "Train loss :  4.550911903381348\n",
            "Validation loss :  6.165074272453785\n",
            "[Epoch 13 / 30]\n",
            "Train loss :  4.596027374267578\n",
            "Validation loss :  6.181639081984758\n",
            "[Epoch 14 / 30]\n",
            "Train loss :  4.536078929901123\n",
            "Validation loss :  6.148149334639311\n",
            "[Epoch 15 / 30]\n",
            "Train loss :  4.463920593261719\n",
            "Validation loss :  6.174063432961702\n",
            "[Epoch 16 / 30]\n",
            "Train loss :  4.187440872192383\n",
            "Validation loss :  6.156722552329302\n",
            "[Epoch 17 / 30]\n",
            "Train loss :  4.667416095733643\n",
            "Validation loss :  6.20366659834981\n",
            "[Epoch 18 / 30]\n",
            "Train loss :  4.351894855499268\n",
            "Validation loss :  6.209446942061186\n",
            "[Epoch 19 / 30]\n",
            "Train loss :  3.9425151348114014\n",
            "Validation loss :  6.175913244485855\n",
            "[Epoch 20 / 30]\n",
            "Train loss :  4.241306781768799\n",
            "Validation loss :  6.226581905037165\n",
            "[Epoch 21 / 30]\n",
            "Train loss :  4.0527448654174805\n",
            "Validation loss :  6.223123540729285\n",
            "[Epoch 22 / 30]\n",
            "Train loss :  4.3940653800964355\n",
            "Validation loss :  6.226823722571135\n",
            "[Epoch 23 / 30]\n",
            "Train loss :  4.35628080368042\n",
            "Validation loss :  6.229899101704359\n",
            "[Epoch 24 / 30]\n",
            "Train loss :  3.9469950199127197\n",
            "Validation loss :  6.161398601531983\n",
            "[Epoch 25 / 30]\n",
            "Train loss :  4.178126811981201\n",
            "Validation loss :  6.224060074239969\n",
            "[Epoch 26 / 30]\n",
            "Train loss :  3.891428232192993\n",
            "Validation loss :  6.206041173636914\n",
            "[Epoch 27 / 30]\n",
            "Train loss :  3.9507904052734375\n",
            "Validation loss :  6.195365469157696\n",
            "[Epoch 28 / 30]\n",
            "Train loss :  3.889268636703491\n",
            "Validation loss :  6.212066601961851\n",
            "[Epoch 29 / 30]\n",
            "Train loss :  4.216470241546631\n",
            "Validation loss :  6.145830248296261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr_SE8gXeyc2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUzX_lk0CZ2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b83294b5-4697-481a-e2b9-668782035dbf"
      },
      "source": [
        "path = \"phase3.pth\" # location to save the model\n",
        "torch.save(model,path) #saving the trained model at defined location\n",
        "# model.train()\n",
        "# model = torch.load(path) #loading the model\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "seq2seq(\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(41006, 250)\n",
              "    (lstm): LSTM(250, 512, num_layers=2, dropout=0.5)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(28207, 250)\n",
              "    (lstm): LSTM(250, 512, num_layers=2, dropout=0.5)\n",
              "    (fc): Linear(in_features=512, out_features=28207, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP8DEIb7-O3-"
      },
      "source": [
        "Function to translate hindi sentences(index vectors) to english sentences(index vector)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X4n-Y3kIfAP"
      },
      "source": [
        "def hin_to_eng_translation(model, device, hindi_num_vec, max_length=50):\n",
        "    hindi_tensor = torch.LongTensor(hindi_num_vec).unsqueeze(1).to(device)\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(hindi_tensor)\n",
        "    eng_num_vec = [E_word2index[\"<sos>\"]] # adding index for <sos> token\n",
        "    eos_idx=E_word2index[\"<eos>\"] # adding index for <eos> token\n",
        "    for _ in range(max_length):\n",
        "        curr_input = torch.LongTensor([eng_num_vec[-1]]).to(device)\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(curr_input, hidden, cell)\n",
        "            curr_output = output.argmax(1).item()\n",
        "        eng_num_vec.append(curr_output) # appending the prediction in english index vector\n",
        "        if (curr_output == eos_idx): # stop generating predictions once eos token is encountered\n",
        "            break\n",
        "    return eng_num_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQJY-N_M--Rl"
      },
      "source": [
        "# 7. Testing the Model \n",
        "Obtaining the reference and prediction files to compute Bleu score and Meteor Score using Evaluation Script. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsVVUsiQ-NXJ"
      },
      "source": [
        "Obtaining the predicted sentences for validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHGXLM_PfLqC"
      },
      "source": [
        "file1 = open(\"validation_prediction.txt\",\"w\") # to store the prediction of validation set\n",
        "csv_file = open(\"validation_ds.csv\",encoding='utf-8')\n",
        "rows = csv.reader(csv_file)\n",
        "for row in rows:\n",
        "    hindi_sentence = row[0]\n",
        "    hindi_sentence_token=[]\n",
        "\n",
        "    # tokenize hindi sentence\n",
        "    if type(hindi_sentence) == str:\n",
        "      for t in indic_tokenize.trivial_tokenize(hindi_sentence):\n",
        "        hindi_sentence_token.append(t)\n",
        "    else:\n",
        "        for t in hindi_sentence:\n",
        "          hindi_sentence_token.append(t)\n",
        "  \n",
        "    hindi_sentence_token.insert(0,'<sos>') # append <sos> token\n",
        "    hindi_sentence_token.append('<eos>') # append <eos> token\n",
        "    hindi_num_vec = []\n",
        "\n",
        "    # generating index vector for hindi sentences\n",
        "    for t in hindi_sentence_token:\n",
        "      hindi_num_vec.append(H_word2index.get(t,2))\n",
        "\n",
        "    # call hin_to_eng_translation function to generate predictions\n",
        "    eng_num_vec = hin_to_eng_translation(model, device, hindi_num_vec, max_length=50)\n",
        "    # eng_num_vec is vector of indices of predicted english sentences. Now, we need to find the words corresponding to these indices\n",
        "\n",
        "    english_sentence_list=[]\n",
        "    for word_idx in eng_num_vec:\n",
        "      english_sentence_list.append(E_index2word.get(word_idx,2)) # index 2 is for <unk>. \n",
        "\n",
        "    english_sentence_list.pop(0) # remove <sos> token\n",
        "    english_sentence_list.pop() # remove <eos> token\n",
        "\n",
        "    # capitalise first letter of first word of predicted english sentence\n",
        "    english_sentence=str(english_sentence_list[0][0].upper()) + str(english_sentence_list[0][1:])\n",
        "\n",
        "    # storing the sentences in form of string (while prediction these words were stored in list that's why now there is need to store them as string)\n",
        "    for string in english_sentence_list[1:]:\n",
        "           english_sentence+=' '+ string\n",
        "    file1.write(english_sentence + \"\\n\")\n",
        "  \n",
        "file1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP0scsSD-veO"
      },
      "source": [
        "Saving the refernce sentences for validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvZ_2KS6R-6A"
      },
      "source": [
        "csv_file = open(\"validation_ds.csv\",encoding='utf-8')\n",
        "rows = csv.reader(csv_file)\n",
        "file = open(\"validation_english.txt\",\"w\")\n",
        "for row in rows:\n",
        "    file.write(row[1]+\"\\n\")\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5FL52a3lSeB"
      },
      "source": [
        "# 8. Generating Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm9H6Xuc_YEa"
      },
      "source": [
        "### Obtaining translation for hindistatements.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7OHLqOmR_j9"
      },
      "source": [
        "file1 = open(\"predicted_text_phase3.txt\",\"w\") # file to store the predictions\n",
        "csv_file = open(\"hindistatements.csv\",encoding='utf-8')\n",
        "rows = csv.reader(csv_file)\n",
        "i=0\n",
        "for row in rows:\n",
        "    if (i==0):  # don't want to translate columns names for prediction\n",
        "      i+=1\n",
        "      continue\n",
        "    hindi_sentence = row[2]\n",
        "    hindi_sentence_token=[]\n",
        "    # tokenize hindi sentence\n",
        "    if type(hindi_sentence) == str:\n",
        "      for t in indic_tokenize.trivial_tokenize(hindi_sentence):\n",
        "        hindi_sentence_token.append(t)\n",
        "    else:\n",
        "        for t in hindi_sentence:\n",
        "          hindi_sentence_token.append(t)\n",
        "  \n",
        "    hindi_sentence_token.insert(0, '<sos>') # append <sos> token\n",
        "    hindi_sentence_token.append('<eos>') # append <eos> token\n",
        "    hindi_num_vec = []\n",
        "\n",
        "    # generating index vector for hindi sentences\n",
        "    for t in hindi_sentence_token:\n",
        "      hindi_num_vec.append(H_word2index.get(t,2))\n",
        "\n",
        "    # call hin_to_eng_translation function to generate predictions\n",
        "    eng_num_vec = hin_to_eng_translation(model, device, hindi_num_vec, max_length=50)\n",
        "    # eng_num_vec is vector of indices of predicted english sentences. Now, we need to find the words corresponding to these indices\n",
        "\n",
        "    english_sentence_list=[] \n",
        "    for word_idx in eng_num_vec:\n",
        "      english_sentence_list.append(E_index2word.get(word_idx,2)) # index 2 is for <unk>. \n",
        "    \n",
        "    english_sentence_list.pop(0) # remove <sos> token\n",
        "    english_sentence_list.pop() # remove <eos> token\n",
        "\n",
        "    # capitalise first letter of first word of predicted english sentence\n",
        "    english_sentence=str(english_sentence_list[0][0].upper()) + str(english_sentence_list[0][1:]) \n",
        "\n",
        "    # storing the sentences in form of string (while prediction these words were stored in list that's why now there is need to store them as string)\n",
        "    for string in english_sentence_list[1:]:\n",
        "           english_sentence+=' '+ string\n",
        "    file1.write(english_sentence + \"\\n\")\n",
        "  \n",
        "file1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEKBy46fsxda"
      },
      "source": [
        "# 9. References\n",
        "\n",
        "[1] [https://arxiv.org/abs/1409.3215t](https://arxiv.org/abs/1409.3215)\n",
        "\n",
        "[2] [https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n"
      ]
    }
  ]
}
